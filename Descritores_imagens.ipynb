{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Descritores-imagens.ipynb",
      "provenance": [],
      "mount_file_id": "1cLWOwnqSf8ixNP_k6fiv_0gOMSf1JANd",
      "authorship_tag": "ABX9TyOj9kl380ExmvsQjXud/vdh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maricamolesi/Iniciacao-Cientifica/blob/master/Descritores_imagens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPd0153IjkP5",
        "colab_type": "text"
      },
      "source": [
        "Código para extrair descritores das imagens do dataset. Os descritores das imagens originais são concatenados com os descritores das imagens corroídas formando uma única matriz de descritores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doje99Ge9HIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from torchvision import datasets\n",
        "from os import listdir\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "import natsort "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH9k6-wmgO3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Carregar imagens\n",
        "PATH_ORIGINAL = '/content/drive/My Drive/ProjetoIC/Brodatz/Brodatz_128x128'\n",
        "LIST_ORIGINAL = natsort.natsorted([f for f in listdir(PATH_ORIGINAL) if isfile(join(PATH_ORIGINAL, f))],reverse=False)\n",
        "\n",
        "PATH_CORRODED = '/content/drive/My Drive/ProjetoIC/v1'\n",
        "LIST_CORRODED = natsort.natsorted([f for f in listdir(PATH_CORRODED) if isfile(join(PATH_CORRODED, f))],reverse=False)\n",
        "\n",
        "FEATURES = []\n",
        "\n",
        "#Carregar a rede pré treinada anteriormente (Notebook: Rede-neural)\n",
        "model = torch.load('/content/drive/My Drive/ProjetoIC/Rede-Neural.pt', map_location='cpu')\n",
        "\n",
        "#Escolha da camada que deseja extrair os descritores\n",
        "layer = model._modules.get('avgpool')\n",
        "\n",
        "#Definir modelo para o modo de avaliação\n",
        "model.eval()\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9lThhKsYLT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Transformações na imagem\n",
        "scaler = transforms.RandomResizedCrop(224)\n",
        "flip = transforms.RandomHorizontalFlip()\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "to_tensor = transforms.ToTensor()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_2egg6WkpDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converter imagens em escala cinza para RGB\n",
        "def pil_loader(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    with Image.open(f) as img:\n",
        "      return img.convert('RGB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxOz6YJJYAnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vector(img):\n",
        "    transformed_img = Variable(normalize(to_tensor(scaler(flip(img)))).unsqueeze(0))\n",
        "\n",
        "    #Vetor de zeros para armazenar os descritores.\n",
        "    #A camada 'avgpool' da rede Resnet50 possui saída de tamanho 2048\n",
        "    my_embedding = torch.zeros(2048)\n",
        "\n",
        "    #Função para copiar a saída da camada\n",
        "    def copy_data(m, i, o):\n",
        "       my_embedding.copy_(o.data.reshape(o.data.size(1)))\n",
        "\n",
        "    h = layer.register_forward_hook(copy_data)\n",
        "    model(transformed_img)\n",
        "    h.remove()\n",
        "    \n",
        "    #Retorna vetor de descritores\n",
        "    return my_embedding.numpy() "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAYSoHiv8_0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extraindo descritores\n",
        "for n in range(1776):\n",
        "  img_original = pil_loader(join(PATH_ORIGINAL, LIST_ORIGINAL[n]))\n",
        "  img_corroded = pil_loader(join(PATH_CORRODED, LIST_CORRODED[n]))\n",
        "\n",
        "  feature_original = get_vector(img_original)\n",
        "  feature_corroded = get_vector(img_corroded)\n",
        "\n",
        "  features = np.vstack([feature_original, feature_corroded]).reshape(1,4096)\n",
        "  features = np.squeeze(features)\n",
        "\n",
        "  FEATURES.append(features)\n",
        "\n",
        "FEATURES = np.array(FEATURES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUClx9AfJVJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Salvar matriz de descritores\n",
        "np.save('/content/drive/My Drive/ProjetoIC/CITA/FEATURES_CORROIDO.npy', FEATURES)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}